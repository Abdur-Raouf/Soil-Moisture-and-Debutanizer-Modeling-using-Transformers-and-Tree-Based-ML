# -*- coding: utf-8 -*-
"""Debutanizer_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W74qzWv46r_MfEkOQHpd7uBHFvbk2svF
"""

import pandas as pd
import statsmodels.api as sm
from scipy import stats

# Read the dataset using regex-based separator for whitespace
df = pd.read_csv('/content/drive/MyDrive/Debutanizer Project/debutanizer_data.txt', sep='\s+', header=None)

# Assign column names
df.columns = ['u1', 'u2', 'u3', 'u4', 'u5', 'u6', 'u7', 'y']

# Calculate z-score of the 'y' column and store it in a new column
df["zscore"] = stats.zscore(df["y"])

# Preview the DataFrame
df1 = df[ (df["zscore"]>3) | (df["zscore"]<-3) ]

df_clean = df[ (df["zscore"]<=3) & (df["zscore"]>=-3) ]


df_clean.isna().sum(axis=1)

df_clean.head()

df = df_clean[['u1', 'u2', 'u3', 'u4', 'u5', 'u6', 'u7', 'y']]
df.head()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# Features and target
X = df[['u1', 'u2', 'u3', 'u4', 'u5', 'u6', 'u7']]
y = df['y']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
dt = DecisionTreeRegressor(random_state=42)

# Define hyperparameters to search
param_grid = {
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': [None, 'sqrt', 'log2']
}

# Use GridSearchCV to find the best parameters
grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,
                           cv=5, scoring='r2', n_jobs=-1, verbose=1)

# Fit the grid search on training data
grid_search.fit(X_train, y_train)

# Best parameters and model
print("Best Parameters:", grid_search.best_params_)
best_model = grid_search.best_estimator_

y_pred = best_model.predict(X_test)
# Evaluate performance
print("\nModel Performance on Test Set by Decision Tree:")
print("RÂ² Score:", r2_score(y_test, y_pred))
print("MAE:", mean_absolute_error(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))



import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# Assuming df is already loaded
X = df[['u1', 'u2', 'u3', 'u4', 'u5', 'u6', 'u7']]
y = df['y']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
xgb = XGBRegressor(objective='reg:squarederror', random_state=42)

print("Best Parameters:", grid_search.best_params_)
best_rf_model = grid_search.best_estimator_

# Predict on test data
y_pred = best_rf_model.predict(X_test)

# Evaluation metrics
print("\nModel Performance on Test Set by Random Forest:")
print("RÂ² Score:", r2_score(y_test, y_pred))
print("MAE:", mean_absolute_error(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import KBinsDiscretizer
import sklearn.metrics as metrics

discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')
y_binned = discretizer.fit_transform(y.values.reshape(-1, 1)).ravel()

X_train, X_test, y_train, y_test = train_test_split(X, y_binned, test_size=0.2, random_state=42)


model = GaussianNB()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluation
print("Model Performance on Test Set by Random Forest:")
print('Accuracy:-',metrics.accuracy_score(y_test, y_pred))
print('Precision:-',metrics.precision_score(y_test, y_pred, average='weighted'))
print('Recall:-',metrics.recall_score(y_test, y_pred, average='weighted'))
print('F1 score:-',metrics.f1_score(y_test, y_pred, average='weighted'))

import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# Assuming df is already loaded
X = df[['u1', 'u2', 'u3', 'u4', 'u5', 'u6', 'u7']]
y = df['y']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
xgb = XGBRegressor(objective='reg:squarederror', random_state=42)

# Define hyperparameter grid
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [2, 3],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1]
}

# Grid Search CV
grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    cv=2,  # Use more folds if dataset is larger
    scoring='r2',
    n_jobs=-1,
    verbose=1
)

# Fit model
grid_search.fit(X_train, y_train)

# Best model and parameters
best_model = grid_search.best_estimator_
print("âœ… Best Parameters:", grid_search.best_params_)

# Predict
y_pred = best_model.predict(X_test)

# Evaluate
print("\nðŸ“Š Evaluation Metrics:")
print("RÂ² Score:", r2_score(y_test, y_pred))
print("Mean Absolute Error (MAE):", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error (MSE):", mean_squared_error(y_test, y_pred))

